\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel} % Для русского языка
\usepackage{graphicx} % Для вставки изображений
\usepackage{float} % Для точного позиционирования графиков
\usepackage{hyperref} % Для кликабельных ссылок
\usepackage{geometry} % Настройка полей
\geometry{left=2.5cm, right=1.5cm, top=2cm, bottom=2cm}

\title{Отчет по проекту: Задачи по случайным графам}
\author{Бахурин Виктор и Стахова Екатерина}
\date{\today}

\begin{document}

\maketitle
\tableofcontents

\section{Введение}
Часть I. Исследование свойств характеристики

\section{Описание кода}
\subsection{Используемые инструменты}
\begin{itemize}
    \item Язык программирования: Python 3.10
    \item Основные библиотеки: numpy, networkx, matplotlib, scikit-learn
    \item Система контроля версий: Git (GitHub/GitLab)
    \item Дополнительные инструменты: Jupyter Notebook, PyCharm, Google Colab
\end{itemize}

\subsection{UML-диаграмма}
Мы не реализовывали свои классы. 
%\begin{figure}[H]
%    \centering
%    \includegraphics[width=0.8\textwidth]{uml_diagram.png}
%    \caption{Диаграмма классов проекта}
%    \label{fig:uml}
%\end{figure}

\subsection{Реализованные алгоритмы}
\subsubsection{$fast\_chromatic\_number()$}
\begin{itemize}
    \item \textbf{Назначение}: Вычисление хроматического числа для случайного графа построенного на данной выборке.
    \item \textbf{Входные данные}: list - выборка
    \item \textbf{Выходные данные}: int - хроматическое число
    \item \textbf{Сложность}: O(nlog(n))
\end{itemize}

\subsubsection{$fast\_max\_independent\_set\_size()$}
\begin{itemize}
    \item \textbf{Назначение}: Вычисление размера максимального независимого множества для случайного графа построенного на данной выборке.
    \item \textbf{Входные данные}: graph - граф
    \item \textbf{Выходные данные}: int - размер независимого множества
    \item \textbf{Сложность}: O(n+m)
\end{itemize}

\subsubsection{$greedy()$}
\begin{itemize}
    \item \textbf{Назначение}: Жадное построение множества А, максимизирующие мощность критерия, при заданной допустимой ошибки первого рода.
    \item \textbf{Входные данные}: $T\_H_0$, $T\_H_1$, $\alpha$ - два набора наблюдений и максимальная допустимая ошибка первого рода.
    \item \textbf{Выходные данные}: A, $current\_error$, power - множество A, ошибка первого рода, мощность критерия. 
    \item \textbf{Сложность}: O(nlog(n))
\end{itemize}

\section{Описание экспериментов}
\subsection{Эксперимент 1}
\subsubsection{Цель}
Исследовать, как ведет себя числовая характеристика $T$ в зависимости
от параметров распределений $θ$ и $υ$, зафиксировав размер выборки и
параметр процедуры построения графа KNN.\\

\subsubsection{Результаты}

Мы получили интересный результат. График для нормального распределения выглядит хаотичнее, чем график для Student-t(ν); в графике Student-t(ν) прослеживается рост $\quad \mathbb{E}[in\_\delta(G)]$ с ростом параметра ν. И еще одно интересное наблюдение: для интересующих нас параметров распределений $v_0$ и $σ_0$ график распределения Student-t(ν) ниже графика нормального распределения.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{гр1.png}
    \caption{$\quad \mathbb{E}[in\_\delta(G)]$ для KNN графа построенного на $Normal(0,\sigma)$}
    \label{fig:uml}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Гр2.png}
    \caption{$\quad \mathbb{E}[in\_\delta(G)]$ для KNN графа построенного на $Student-t(ν)$}
    \label{fig:uml}
\end{figure}

Графики экспоненциального и gamma-распределения выглядят хаотично. Не прослеживается никакая зависимость от параметров.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{граф1.png}
    \caption{Максимальная степень вершины для KNN графа построенного на $Exp(\sigma)$}
    \label{fig:uml}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Граф2.png}
    \caption{Максимальная степень вершины для KNN графа построенного на $Gamma(ν)$}
    \label{fig:uml}
\end{figure}

\subsection{Эксперимент 2}
\subsubsection{Цель}
Исследовать, как ведет себя числовая характеристика $T$ в зависимости
от параметров распределений $θ$ и $υ$, зафиксировав размер выборки и
параметр процедуры построения графа dist.\\

\subsubsection{Результаты}

Характеристика χ(G) на дистанционном графе показывает разные результаты для разных выборок. Для нормального распределения с ростом параметра $\sigma$ хроматическое число убывает, а для распределения Student-t(ν) с ростом параметра v $\chi(G)$ наоборот растет.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Гр3.png}
    \caption{$\quad \mathbb{E}[\chi(G)]$ для dist графа построенного на $Normal(0,\sigma)$}
    \label{fig:uml}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Гр4.png}
    \caption{$\quad \mathbb{E}[\chi(G)]$ для dist графа построенного на $Student-t(ν)$}
    \label{fig:uml}
\end{figure}

Размер максимального независимого множества убывает с увеличением параметра q и v. Однако для гамма-распределения зависимость несколько более хаотичная.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Граф3.png}
    \caption{Размер макс. независимого множества для dist графа построенного на $Exp(\sigma)$}
    \label{fig:uml}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Граф4.png}
    \caption{Размер макс. независимого множества для dist графа построенного на $Gamma(ν)$}
    \label{fig:uml}
\end{figure}

\subsection{Эксперимент 3}
\subsubsection{Цель}
Исследовать, как ведет себя числовая характеристика T в зависимости
от параметров процедуры построения графа KNN и размера выборки при
фиксированных значениях $\theta = \theta_0$ и $v = v_0$.\\
\subsubsection{Результаты}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Гр5.png}
    \caption{$\quad \mathbb{E}[in\_\delta(G)]$ для KNN графа}
    \label{fig:uml}
\end{figure}
График для Normal выше, чем график для Student. Это может помочь в проверке истинности $H_0$ и $H_1$. \\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Граф5.png}
    \caption{Макс. степень вершины для KNN графа}
    \label{fig:uml}
\end{figure}

При больших k график Гамма-распределения находится выше экспоненциального распределения.

\subsection{Эксперимент 4}
\subsubsection{Цель}
Исследовать, как ведет себя числовая характеристика T в зависимости
от параметров процедуры построения дистанционного графа и размера выборки при
фиксированных значениях $\theta = \theta_0$ и $v = v_0$.\\
\subsubsection{Результаты}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Гр6.png}
    \caption{$\quad \mathbb{E}[\chi(G)]$ для dist графа}
    \label{fig:uml}
\end{figure}
К сожалению, данные графики не сильно отличаются, в среднем график для Student-t(ν) ниже, чем график $Normal(0,\sigma)$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Граф6.png}
    \caption{Размер макс. независимого множества для dist графа}
    \label{fig:uml}
\end{figure}

Экспонента в среднем выше, чем гамма-распределение.

\subsection{Промежуточный вывод}
Если обобщить результаты, полученные в предыдущих пунктах, то можно заметить, что каждая из характеристик показывает разные значения на случайных графах, построенных на распределениях $Student-t(ν)$ и нормальном распределении $Normal(0,\sigma)$. Это означает, что существует возможность использовать их для проверки истинности гипотез $H_0$ и $H_1$.\\
Аналогичные рассуждения верны для экспоненциального и гамма распределений.

\subsection{Эксперимент 5}
\subsubsection{Цель}
Построить множество $A$ в предположении $\theta = \theta_0$ и $v = v_0$ при максимальной допустимой вероятности ошибки первого рода $\alpha$ = 0.055.
Оценить мощность полученного критерия.
\subsubsection{Результаты}
Для каждой характеристики удалось построить множество A.\\
Используя характеристику $in\_\delta(G)$ на графе KNN получен следующий результат:\\
Ошибка первого рода $\alpha = 0.035.$\\
Мощность полученного критерия 0.717.\\
Используя характеристику $\chi(G)$ на графе dist получен следующий результат:\\
Ошибка первого рода $\alpha = 0.045.$\\
Мощность полученного критерия 0.594.\\
В первом случае результат значительно лучше.\\

\\

Используя характеристику макс. степень вершины в графе knn получен следующий результат:\\
Ошибка первого рода $\alpha = 0.039.$\\
Мощность полученного критерия 0.303.\\

Используя характеристику размер макс. независимого множества в графе dist получен следующий результат:\\
Ошибка первого рода $\alpha = 0.053.$\\
Мощность полученного критерия 0.314.\\

\subsection{Эксперимент 6}
\subsubsection{Цель}
Исследование важности характеристик, как признаков классификации. Узнать, меняется ли важность характеристик с ростом n.\\
В качестве графа был выбран dist, так как он неориентированный.\\
\subsubsection{Результаты}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{В2.png}
    \caption{Важность признаков на dist графе}
    \label{fig:uml}
\end{figure}
Мы видим, что каждый из признаков (хроматическое число и мин. степень) дает хорошую мощность критерия на значениях n от 50. При этом мы также видим значение dist, на котором было получено лучшее значение мощности критерия. 

\subsubsection{Результаты}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{К1.png}
    \caption{Важность признаков на dist графе}
    \label{fig:uml}
\end{figure}

В то же время признаки макс. степень вершины и макс. размер независимого множества дают очень маленькое значение на коротких выборках (до 50) и хорошие показатели, близкие к 1, на выборках (n от 150).

\subsection{Эксперимент 7}
\subsubsection{Цель}
Применить разные классификационные алгоритмы и оценить метрики качества.
В качестве классификаторов были выбраны следующие: LogisticRegression, RandomForestClassifier и MyClassifier.\\
\subsubsection{Результаты}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{В1.png}
    \caption{Значения метрик}
    \label{fig:uml}
\end{figure}


Мы видим, что используя характеристики хроматическое число и мин. степень вершины, мы получаем точность $95\%$ уже при n=75. Это хороший результат. Стоит отметить, что собственный классификатор показывает не очень хорошую, относительно других классификаторов, метрику recall, это вызвано его реализацией: в случае, когда характеристики указывают на разные гипотезы, ответ выбирается равновероятно, это и дает данную погрешность. В остальных метриках классификаторы ведут себя схоже: RandomForestClassifier чуть лучше, LogisticRegression чуть хуже, MyClassifier еще чуть хуже.  

\subsubsection{Результаты}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{К2.png}
    \caption{Значения метрик}
    \label{fig:uml}
\end{figure}

Для признаков макс. степень вершины и макс. размер независимого множества показатели метрик уже значительно ниже, не превосходят 0,65 \%. Это может быть связано с применением жадного (но не точного) алгоритма для поиска макс. размера независимого множества, используемого для ускорения вычислений. Однако на метрике Recall MyClassifier выдает результат близкий к 1.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{В3.png}
    \caption{Значения дисперсий метрик}
    \label{fig:uml}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{К3.png}
    \caption{Значения дисперсий метрик}
    \label{fig:uml}
\end{figure}

Посмотрим на дисперсию метрик, для всех метрик и классификаторов верно, что коэффициент вариации находится в диапазоне от $10\%$ до $0\%$ и уменьшается с ростом n (исключение метрика recall и MyClassifier, но причину этого я уже описывал выше). Хороший ли это показатель, зависит от задачи. 

\subsection{Оценка собственного классификатора}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{В4.png}
    \caption{Ошибка первого рода и мощность критерия}
    \label{fig:uml}
\end{figure}

По графику заметно, что при значении ошибки первого рода в диапазоне  0.055 мы получили хорошие результаты мощности критерия при n от 50, мощность критерия в районе $90\%$. Я считаю, что для двух характеристик это неплохой результат. Возможно, если реализовывать классификатор другим способом, можно добиться лучшего(например, если руками реализовать RandomForestClassifier), но для столь наивной реализации результат неплох.\\


%\section{Заключение}
%Итоговые выводы по проекту:
%\begin{itemize}
%    \item Основные достижения
%    \item Области для улучшения
%    \item Возможные направления развития
%\end{itemize}

\end{document}
